"""
XGBoost Forecasting cho d·ª± b√°o b√°n h√†ng v√† t·ªìn kho
S·ª≠ d·ª•ng Optuna cho hyperparameter tuning
"""

import pandas as pd
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Any
import logging
import joblib
import os
import json
import warnings

import xgboost as xgb
from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_absolute_percentage_error

# Optuna cho Bayesian Optimization
try:
    import optuna
    from optuna.samplers import TPESampler
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False
    warnings.warn("Optuna not installed. Tuning will use RandomizedSearchCV instead.")

from db_connectors import PostgreSQLConnector, ClickHouseConnector
from email_notifier import EmailNotifier, get_notifier

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class SalesForecaster:
    """D·ª± b√°o doanh s·ªë b√°n h√†ng s·ª≠ d·ª•ng XGBoost"""
    
    def __init__(self, model_dir: str = '/app/models', enable_email: bool = True):
        self.model_dir = model_dir
        os.makedirs(model_dir, exist_ok=True)
        
        self.pg = PostgreSQLConnector(
            host=os.getenv('POSTGRES_HOST', 'localhost'),
            database=os.getenv('POSTGRES_DB', 'retail_db'),
            user=os.getenv('POSTGRES_USER', 'retail_user'),
            password=os.getenv('POSTGRES_PASSWORD', 'retail_password')
        )
        
        self.ch = ClickHouseConnector(
            host=os.getenv('CLICKHOUSE_HOST', 'localhost'),
            database=os.getenv('CLICKHOUSE_DB', 'retail_dw'),
            user=os.getenv('CLICKHOUSE_USER', 'default'),
            password=os.getenv('CLICKHOUSE_PASSWORD', 'clickhouse_password')
        )
        
        self.models = {}
        self.metrics = {}
        self.studies = {}  # L∆∞u Optuna studies
        
        # Kh·ªüi t·∫°o email notifier
        self.email_notifier = None
        if enable_email:
            try:
                self.email_notifier = get_notifier()
                logger.info("üìß Email notifier ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Kh√¥ng th·ªÉ kh·ªüi t·∫°o email notifier: {e}")
        self.feature_cols = [
            'day_of_week', 'day_of_month', 'month', 'week_of_year',
            'is_weekend', 'is_month_start', 'is_month_end', 'is_holiday',
            'lag_1_quantity', 'lag_7_quantity', 'lag_14_quantity', 'lag_30_quantity',
            'lag_1_revenue', 'lag_7_revenue', 'lag_14_revenue',
            'rolling_mean_7_quantity', 'rolling_std_7_quantity',
            'rolling_mean_30_quantity', 'quantity_growth',
            'branch_encoded', 'category1_encoded', 'category2_encoded'
        ]
    
    def load_historical_data(self, days: int = 365) -> pd.DataFrame:
        """Load d·ªØ li·ªáu l·ªãch s·ª≠ t·ª´ ClickHouse"""
        query = f"""
        SELECT
            ngay,
            chi_nhanh,
            ma_hang,
            nhom_hang_cap_1,
            nhom_hang_cap_2,
            SUM(doanh_thu) as daily_revenue,
            SUM(so_luong) as daily_quantity,
            SUM(loi_nhuan_gop) as daily_profit,
            COUNT(DISTINCT ma_giao_dich) as transaction_count
        FROM retail_dw.fact_transactions
        WHERE ngay >= today() - {days}
        GROUP BY ngay, chi_nhanh, ma_hang, nhom_hang_cap_1, nhom_hang_cap_2
        ORDER BY ngay
        """
        
        df = self.ch.query(query)
        df['ngay'] = pd.to_datetime(df['ngay'])
        return df
    
    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """T·∫°o features cho model"""
        df = df.copy()
        
        # Time-based features
        df['day_of_week'] = df['ngay'].dt.dayofweek
        df['day_of_month'] = df['ngay'].dt.day
        df['month'] = df['ngay'].dt.month
        df['week_of_year'] = df['ngay'].dt.isocalendar().week
        df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)
        df['is_month_start'] = df['ngay'].dt.is_month_start.astype(int)
        df['is_month_end'] = df['ngay'].dt.is_month_end.astype(int)
        
        # Vietnamese holidays (simplified)
        df['is_holiday'] = self._is_vietnamese_holiday(df['ngay']).astype(int)
        
        # Lag features
        df = df.sort_values(['chi_nhanh', 'ma_hang', 'ngay'])
        for lag in [1, 7, 14, 30]:
            df[f'lag_{lag}_quantity'] = df.groupby(['chi_nhanh', 'ma_hang'])['daily_quantity'].shift(lag)
            df[f'lag_{lag}_revenue'] = df.groupby(['chi_nhanh', 'ma_hang'])['daily_revenue'].shift(lag)
        
        # Rolling statistics
        for window in [7, 14, 30]:
            df[f'rolling_mean_{window}_quantity'] = df.groupby(['chi_nhanh', 'ma_hang'])['daily_quantity'] \
                .transform(lambda x: x.rolling(window, min_periods=1).mean())
            df[f'rolling_std_{window}_quantity'] = df.groupby(['chi_nhanh', 'ma_hang'])['daily_quantity'] \
                .transform(lambda x: x.rolling(window, min_periods=1).std())
        
        # Growth rate
        df['quantity_growth'] = df.groupby(['chi_nhanh', 'ma_hang'])['daily_quantity'].pct_change()
        
        # Encoding cho categorical
        df['branch_encoded'] = pd.Categorical(df['chi_nhanh']).codes
        df['category1_encoded'] = pd.Categorical(df['nhom_hang_cap_1']).codes
        df['category2_encoded'] = pd.Categorical(df['nhom_hang_cap_2']).codes
        
        return df
    
    def _is_vietnamese_holiday(self, dates: pd.Series) -> pd.Series:
        """Ki·ªÉm tra ng√†y l·ªÖ Vi·ªát Nam (simplified)"""
        holidays = []
        for date in dates:
            month_day = (date.month, date.day)
            # T·∫øt (gi·∫£ ƒë·ªãnh)
            if date.month == 1 and date.day <= 5:
                holidays.append(True)
            # 30/4
            elif month_day == (4, 30):
                holidays.append(True)
            # 1/5
            elif month_day == (5, 1):
                holidays.append(True)
            # 2/9
            elif month_day == (9, 2):
                holidays.append(True)
            else:
                holidays.append(False)
        return pd.Series(holidays, index=dates.index)
    
    def train_model(self, df: pd.DataFrame, target_col: str = 'daily_quantity') -> xgb.XGBRegressor:
        """Train XGBoost model v·ªõi default hyperparameters (kh√¥ng tuning)"""
        df_clean = df.dropna()
        
        # Ki·ªÉm tra d·ªØ li·ªáu
        if len(df_clean) == 0:
            logger.error(f"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu sau khi lo·∫°i b·ªè NA cho target '{target_col}'")
            return xgb.XGBRegressor(
                objective='reg:squarederror',
                n_estimators=100,
                max_depth=3,
                random_state=42
            )
        
        # Ch·ªâ s·ª≠ d·ª•ng c√°c features c√≥ s·∫µn trong dataframe
        available_features = [col for col in self.feature_cols if col in df_clean.columns]
        if not available_features:
            logger.error(f"‚ùå Kh√¥ng c√≥ features n√†o ph√π h·ª£p trong d·ªØ li·ªáu")
            return xgb.XGBRegressor(
                objective='reg:squarederror',
                n_estimators=100,
                max_depth=3,
                random_state=42
            )
        
        if len(available_features) < len(self.feature_cols):
            logger.warning(f"‚ö†Ô∏è Ch·ªâ c√≥ {len(available_features)}/{len(self.feature_cols)} features kh·∫£ d·ª•ng")
        
        X = df_clean[available_features]
        y = df_clean[target_col]
        
        # ƒêi·ªÅu ch·ªânh n_splits d·ª±a tr√™n s·ªë l∆∞·ª£ng d·ªØ li·ªáu
        n_splits = min(5, max(2, len(X) // 6))
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        # Default model
        model = xgb.XGBRegressor(
            objective='reg:squarederror',
            n_estimators=500,
            max_depth=6,
            learning_rate=0.1,
            subsample=0.8,
            colsample_bytree=0.8,
            random_state=42,
            n_jobs=-1
        )
        
        # Train
        model.fit(X, y)
        
        # Cross-validation
        cv_scores = []
        for train_idx, val_idx in tscv.split(X):
            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]
            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
            
            model.fit(X_train, y_train)
            y_pred = model.predict(X_val)
            
            mape = mean_absolute_percentage_error(y_val, y_pred)
            cv_scores.append(mape)
        
        if cv_scores:
            logger.info(f"CV MAPE ({n_splits} folds): {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})")
        else:
            logger.warning("Kh√¥ng th·ªÉ t√≠nh CV scores do thi·∫øu d·ªØ li·ªáu")
        
        # Feature importance
        importance = pd.DataFrame({
            'feature': available_features,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        logger.info(f"Top 5 features: {importance.head().to_dict('records')}")
        
        return model

    def train_model_optuna(self, df: pd.DataFrame, target_col: str = 'daily_quantity', 
                          n_trials: int = 50, timeout: int = 600) -> xgb.XGBRegressor:
        """
        Train XGBoost v·ªõi Bayesian Optimization s·ª≠ d·ª•ng Optuna
        
        Args:
            df: DataFrame v·ªõi features
            target_col: C·ªôt target c·∫ßn d·ª± b√°o
            n_trials: S·ªë l·∫ßn th·ª≠ hyperparameters
            timeout: Th·ªùigian t·ªëi ƒëa (gi√¢y)
        
        Returns:
            XGBRegressor v·ªõi best hyperparameters
        """
        if not OPTUNA_AVAILABLE:
            logger.warning("Optuna not available, falling back to RandomizedSearchCV")
            return self.train_model_random_search(df, target_col)
        
        df_clean = df.dropna()
        
        # Ki·ªÉm tra d·ªØ li·ªáu sau khi dropna
        if len(df_clean) == 0:
            logger.error(f"‚ùå Kh√¥ng c√≥ d·ªØ li·ªáu sau khi lo·∫°i b·ªè NA cho target '{target_col}'")
            # Tr·∫£ v·ªÅ model m·∫∑c ƒë·ªãnh
            return xgb.XGBRegressor(
                objective='reg:squarederror',
                n_estimators=100,
                max_depth=3,
                random_state=42
            )
        
        # Ch·ªâ s·ª≠ d·ª•ng c√°c features c√≥ s·∫µn trong dataframe
        available_features = [col for col in self.feature_cols if col in df_clean.columns]
        if not available_features:
            logger.error(f"‚ùå Kh√¥ng c√≥ features n√†o ph√π h·ª£p trong d·ªØ li·ªáu")
            return xgb.XGBRegressor(
                objective='reg:squarederror',
                n_estimators=100,
                max_depth=3,
                random_state=42
            )
        
        if len(available_features) < len(self.feature_cols):
            logger.warning(f"‚ö†Ô∏è Ch·ªâ c√≥ {len(available_features)}/{len(self.feature_cols)} features kh·∫£ d·ª•ng cho Optuna")
        
        X = df_clean[available_features]
        y = df_clean[target_col]
        
        # Ki·ªÉm tra s·ªë l∆∞·ª£ng m·∫´u
        min_samples_required = 30  # T·ªëi thi·ªÉu cho TimeSeriesSplit(n_splits=5)
        if len(X) < min_samples_required:
            logger.warning(f"‚ö†Ô∏è √çt d·ªØ li·ªáu ({len(X)} samples) cho target '{target_col}'. S·ª≠ d·ª•ng model m·∫∑c ƒë·ªãnh.")
            return self.train_model(df_clean, target_col)
        
        # Time series split cho CV - gi·∫£m s·ªë folds n·∫øu d·ªØ li·ªáu √≠t
        n_splits = min(5, len(X) // 6)  # ƒê·∫£m b·∫£o m·ªói fold c√≥ √≠t nh·∫•t 6 samples
        if n_splits < 2:
            n_splits = 2
        tscv = TimeSeriesSplit(n_splits=n_splits)
        
        # Split train/val cho early stopping
        split_idx = int(len(X) * 0.8)
        if split_idx < 10:  # ƒê·∫£m b·∫£o train set c√≥ ƒë·ªß d·ªØ li·ªáu
            split_idx = max(10, int(len(X) * 0.5))
        
        X_train_full, X_val = X.iloc[:split_idx], X.iloc[split_idx:]
        y_train_full, y_val = y.iloc[:split_idx], y.iloc[split_idx:]
        
        # Ki·ªÉm tra l·∫°i train set
        if len(X_train_full) < n_splits * 2:
            logger.warning(f"‚ö†Ô∏è Kh√¥ng ƒë·ªß d·ªØ li·ªáu cho CV. Chuy·ªÉn sang train model ƒë∆°n gi·∫£n.")
            return self.train_model(df_clean, target_col)
        
        def objective(trial):
            """Objective function cho Optuna"""
            
            params = {
                'objective': 'reg:squarederror',
                'random_state': 42,
                'n_jobs': -1,
                'verbosity': 0,
                
                # Tree structure - quan tr·ªçng nh·∫•t cho time series
                'max_depth': trial.suggest_int('max_depth', 3, 10),
                'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
                'gamma': trial.suggest_float('gamma', 0.0, 0.5, step=0.1),
                
                # Sampling - ch·ªëng overfitting
                'subsample': trial.suggest_float('subsample', 0.6, 1.0, step=0.1),
                'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0, step=0.1),
                'colsample_bylevel': trial.suggest_float('colsample_bylevel', 0.6, 1.0, step=0.1),
                
                # Regularization
                'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 10.0, log=True),
                'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),
                
                # Learning - s·ªë c√¢y s·∫Ω ƒë∆∞·ª£c ƒëi·ªÅu ch·ªânh b·ªüi early stopping
                'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
                'n_estimators': 2000,  # Cao, s·∫Ω early stop
            }
            
            model = xgb.XGBRegressor(**params)
            
            # Cross-validation v·ªõi TimeSeriesSplit
            cv_scores = []
            for train_idx, valid_idx in tscv.split(X_train_full):
                X_train_cv, X_valid_cv = X_train_full.iloc[train_idx], X_train_full.iloc[valid_idx]
                y_train_cv, y_valid_cv = y_train_full.iloc[train_idx], y_train_full.iloc[valid_idx]
                
                model.fit(
                    X_train_cv, y_train_cv,
                    eval_set=[(X_valid_cv, y_valid_cv)],
                    early_stopping_rounds=30,
                    verbose=False
                )
                
                y_pred = model.predict(X_valid_cv)
                mape = mean_absolute_percentage_error(y_valid_cv, y_pred)
                cv_scores.append(mape)
            
            return np.mean(cv_scores)
        
        # T·∫°o study v·ªõi pruning
        study_name = f"{target_col}_study"
        study = optuna.create_study(
            direction='minimize',
            sampler=TPESampler(seed=42),
            pruner=optuna.pruners.MedianPruner(n_startup_trials=10, n_warmup_steps=20),
            study_name=study_name
        )
        
        logger.info(f"üîç Starting Optuna tuning for '{target_col}' with {n_trials} trials...")
        study.optimize(objective, n_trials=n_trials, timeout=timeout, show_progress_bar=True)
        
        # Log k·∫øt qu·∫£
        logger.info(f"‚úÖ Best MAPE: {study.best_value:.4f}")
        logger.info(f"üéØ Best params: {study.best_params}")
        
        # Train final model v·ªõi best params + early stopping
        best_params = study.best_params.copy()
        best_params.update({
            'objective': 'reg:squarederror',
            'random_state': 42,
            'n_jobs': -1,
            'n_estimators': 2000  # High, early stopping will find optimal
        })
        
        final_model = xgb.XGBRegressor(**best_params)
        final_model.fit(
            X_train_full, y_train_full,
            eval_set=[(X_val, y_val)],
            early_stopping_rounds=50,
            verbose=False
        )
        
        # Validation metrics
        y_pred = final_model.predict(X_val)
        val_mape = mean_absolute_percentage_error(y_val, y_pred)
        val_rmse = np.sqrt(mean_squared_error(y_val, y_pred))
        val_mae = mean_absolute_error(y_val, y_pred)
        
        logger.info(f"üìä Validation MAPE: {val_mape:.4f}")
        logger.info(f"üìä Validation RMSE: {val_rmse:.4f}")
        logger.info(f"üìä Validation MAE: {val_mae:.4f}")
        
        # Feature importance
        importance = pd.DataFrame({
            'feature': available_features,
            'importance': final_model.feature_importances_
        }).sort_values('importance', ascending=False)
        logger.info(f"üèÜ Top 5 features:\n{importance.head().to_string()}")
        
        # L∆∞u metrics v√† study
        self.metrics[target_col] = {
            'tuning_method': 'optuna',
            'best_params': study.best_params,
            'cv_mape': study.best_value,
            'val_mape': val_mape,
            'val_rmse': val_rmse,
            'val_mae': val_mae,
            'best_iteration': final_model.best_iteration,
            'n_trials': len(study.trials)
        }
        self.studies[target_col] = study
        
        # L∆∞u study
        study_path = os.path.join(self.model_dir, f'{target_col}_optuna_study.pkl')
        joblib.dump(study, study_path)
        logger.info(f"üíæ Study saved to {study_path}")
        
        return final_model

    def train_model_random_search(self, df: pd.DataFrame, target_col: str = 'daily_quantity',
                                   n_iter: int = 20) -> xgb.XGBRegressor:
        """Fallback: RandomizedSearchCV khi Optuna kh√¥ng available"""
        
        df_clean = df.dropna()
        X = df_clean[self.feature_cols]
        y = df_clean[target_col]
        
        tscv = TimeSeriesSplit(n_splits=5)
        
        param_distributions = {
            'max_depth': [3, 5, 6, 8, 10],
            'learning_rate': [0.01, 0.05, 0.1, 0.15, 0.2],
            'n_estimators': [200, 500, 800, 1000, 1500],
            'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],
            'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],
            'min_child_weight': [1, 3, 5, 7],
            'gamma': [0, 0.1, 0.2, 0.3],
            'reg_alpha': [0, 0.1, 1, 10],
            'reg_lambda': [0.1, 1, 5, 10]
        }
        
        base_model = xgb.XGBRegressor(
            objective='reg:squarederror',
            random_state=42,
            n_jobs=-1
        )
        
        logger.info(f"üîç Starting RandomizedSearchCV for '{target_col}'...")
        random_search = RandomizedSearchCV(
            estimator=base_model,
            param_distributions=param_distributions,
            n_iter=n_iter,
            scoring='neg_mean_absolute_percentage_error',
            cv=tscv,
            verbose=1,
            random_state=42,
            n_jobs=-1
        )
        
        random_search.fit(X, y)
        
        logger.info(f"‚úÖ Best CV MAPE: {-random_search.best_score_:.4f}")
        logger.info(f"üéØ Best params: {random_search.best_params_}")
        
        self.metrics[target_col] = {
            'tuning_method': 'random_search',
            'best_params': random_search.best_params_,
            'cv_mape': -random_search.best_score_,
            'n_iter': n_iter
        }
        
        return random_search.best_estimator_
    
    def train_all_models(self, use_tuning: bool = True, tuning_method: str = 'optuna',
                         n_trials: int = 50, days: int = 365, send_email: bool = True) -> Dict:
        """
        Train models cho t·∫•t c·∫£ levels
        
        Args:
            use_tuning: C√≥ s·ª≠ d·ª•ng hyperparameter tuning kh√¥ng
            tuning_method: 'optuna' ho·∫∑c 'random_search'
            n_trials: S·ªë l·∫ßn th·ª≠ nghi·ªám hyperparameters
            days: S·ªë ng√†y d·ªØ li·ªáu l·ªãch s·ª≠ ƒë·ªÉ train
            send_email: C√≥ g·ª≠i email th√¥ng b√°o kh√¥ng
        
        Returns:
            Dict ch·ª©a metrics c·ªßa t·∫•t c·∫£ models
        """
        import time
        start_time = time.time()
        
        logger.info("=" * 60)
        logger.info("üöÄ B·∫ÆT ƒê·∫¶U TRAINING PIPELINE")
        logger.info("=" * 60)
        
        # Load data
        logger.info(f"üì• Loading {days} days of historical data...")
        df = self.load_historical_data(days=days)
        logger.info(f"‚úÖ Loaded {len(df):,} rows")
        
        # Feature engineering
        logger.info("üîß Creating features...")
        df_features = self.create_features(df)
        logger.info(f"‚úÖ Created {len(self.feature_cols)} features")
        
        # Ch·ªçn training function
        if use_tuning:
            if tuning_method == 'optuna' and OPTUNA_AVAILABLE:
                train_func = lambda df, target: self.train_model_optuna(df, target, n_trials=n_trials)
                logger.info(f"üéØ Using Optuna tuning with {n_trials} trials")
            else:
                train_func = lambda df, target: self.train_model_random_search(df, target, n_iter=n_trials)
                logger.info(f"üéØ Using Random Search with {n_trials} iterations")
        else:
            train_func = self.train_model
            logger.info("‚ö° Using default hyperparameters (no tuning)")
        
        # Model 1: Product-level quantity forecast
        logger.info("\n" + "-" * 40)
        logger.info("üì¶ Model 1: Product-Level Quantity Forecast")
        logger.info("-" * 40)
        self.models['product_quantity'] = train_func(df_features, 'daily_quantity')
        
        # Model 2: Revenue forecast
        logger.info("\n" + "-" * 40)
        logger.info("üí∞ Model 2: Revenue Forecast")
        logger.info("-" * 40)
        self.models['product_revenue'] = train_func(df_features, 'daily_revenue')
        
        # Model 3: Category-level
        logger.info("\n" + "-" * 40)
        logger.info("üìä Model 3: Category-Level Forecast")
        logger.info("-" * 40)
        category_df = df_features.groupby(['ngay', 'nhom_hang_cap_1']).agg({
            'daily_quantity': 'sum',
            'daily_revenue': 'sum',
            'day_of_week': 'first',
            'day_of_month': 'first',
            'month': 'first',
            'week_of_year': 'first',
            'is_weekend': 'first',
            'is_month_start': 'first',
            'is_month_end': 'first',
            'is_holiday': 'first'
        }).reset_index()
        
        # T·∫°o l·∫°i features cho category-level (kh√¥ng c·∫ßn lag features ph·ª©c t·∫°p)
        category_df = category_df.sort_values(['nhom_hang_cap_1', 'ngay'])
        
        # Lag features cho category
        for lag in [1, 7, 14]:
            category_df[f'lag_{lag}_quantity'] = category_df.groupby('nhom_hang_cap_1')['daily_quantity'].shift(lag)
            category_df[f'lag_{lag}_revenue'] = category_df.groupby('nhom_hang_cap_1')['daily_revenue'].shift(lag)
        
        # Rolling statistics
        for window in [7, 14]:
            category_df[f'rolling_mean_{window}_quantity'] = category_df.groupby('nhom_hang_cap_1')['daily_quantity'] \
                .transform(lambda x: x.rolling(window, min_periods=1).mean())
        
        # Growth rate
        category_df['quantity_growth'] = category_df.groupby('nhom_hang_cap_1')['daily_quantity'].pct_change()
        
        # Encoding cho category
        category_df['category1_encoded'] = pd.Categorical(category_df['nhom_hang_cap_1']).codes
        
        # Th√™m c√°c c·ªôt c·∫ßn thi·∫øt nh∆∞ng kh√¥ng c√≥ gi√° tr·ªã cho category-level
        category_df['chi_nhanh'] = 'ALL_BRANCHES'  # Dummy value
        category_df['ma_hang'] = 'ALL_PRODUCTS'    # Dummy value
        category_df['nhom_hang_cap_2'] = 'ALL_CAT2'  # Dummy value
        category_df['branch_encoded'] = 0
        category_df['category2_encoded'] = 0
        
        # Th√™m c√°c lag features c√≤n thi·∫øu v·ªõi gi√° tr·ªã 0
        for col in self.feature_cols:
            if col not in category_df.columns:
                category_df[col] = 0
        
        self.models['category_quantity'] = train_func(category_df, 'daily_quantity')
        
        # L∆∞u models
        logger.info("\n" + "-" * 40)
        logger.info("üíæ Saving models...")
        logger.info("-" * 40)
        for name, model in self.models.items():
            model_path = os.path.join(self.model_dir, f'{name}_model.pkl')
            joblib.dump(model, model_path)
            logger.info(f"‚úÖ Saved: {name} ‚Üí {model_path}")
        
        # L∆∞u metrics
        metrics_path = os.path.join(self.model_dir, 'training_metrics.json')
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(self.metrics, f, indent=2, ensure_ascii=False)
        logger.info(f"‚úÖ Metrics saved to {metrics_path}")
        
        # Summary
        logger.info("\n" + "=" * 60)
        logger.info("üìä TRAINING SUMMARY")
        logger.info("=" * 60)
        for model_name, metrics in self.metrics.items():
            cv_mape = metrics.get('cv_mape', 'N/A')
            val_mape = metrics.get('val_mape', 'N/A')
            logger.info(f"üìà {model_name}:")
            logger.info(f"   CV MAPE: {cv_mape:.4f}" if isinstance(cv_mape, float) else f"   CV MAPE: {cv_mape}")
            if isinstance(val_mape, float):
                logger.info(f"   Val MAPE: {val_mape:.4f}")
        logger.info("=" * 60)
        
        # G·ª≠i email th√¥ng b√°o
        training_duration = time.time() - start_time
        if send_email and self.email_notifier:
            try:
                logger.info("üìß ƒêang g·ª≠i email th√¥ng b√°o training...")
                success = self.email_notifier.send_training_report(
                    metrics=self.metrics,
                    training_duration=training_duration,
                    model_dir=self.model_dir
                )
                if success:
                    logger.info("‚úÖ ƒê√£ g·ª≠i email training report th√†nh c√¥ng")
                else:
                    logger.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ g·ª≠i email training report")
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi g·ª≠i email: {e}")
        
        return self.metrics

    def get_tuning_summary(self, target_col: str = None) -> pd.DataFrame:
        """Tr·∫£ v·ªÅ summary c·ªßa Optuna study"""
        if not OPTUNA_AVAILABLE or not self.studies:
            return pd.DataFrame()
        
        if target_col and target_col in self.studies:
            study = self.studies[target_col]
            trials_df = study.trials_dataframe()
            return trials_df
        
        # T·ªïng h·ª£p t·∫•t c·∫£ studies
        summaries = []
        for name, study in self.studies.items():
            summaries.append({
                'model': name,
                'best_mape': study.best_value,
                'n_trials': len(study.trials),
                'best_params': str(study.best_params)
            })
        return pd.DataFrame(summaries)
    
    def predict_next_week(self) -> pd.DataFrame:
        """D·ª± b√°o cho tu·∫ßn t·ªõi"""
        # Load models n·∫øu ch∆∞a c√≥
        if not self.models:
            for name in ['product_quantity', 'product_revenue', 'category_quantity']:
                model_path = os.path.join(self.model_dir, f'{name}_model.pkl')
                if os.path.exists(model_path):
                    self.models[name] = joblib.load(model_path)
        
        # T·∫°o future dates
        future_dates = pd.date_range(
            start=datetime.now().date() + timedelta(days=1),
            periods=7,
            freq='D'
        )
        
        # L·∫•y danh s√°ch s·∫£n ph·∫©m
        products_query = """
        SELECT DISTINCT chi_nhanh, ma_hang, nhom_hang_cap_1, nhom_hang_cap_2
        FROM retail_dw.fact_transactions
        WHERE ngay >= today() - 30
        """
        products = self.ch.query(products_query)
        
        forecasts = []
        
        for date in future_dates:
            for _, product in products.iterrows():
                # T·∫°o features cho prediction
                features = pd.DataFrame({
                    'ngay': [date],
                    'chi_nhanh': [product['chi_nhanh']],
                    'ma_hang': [product['ma_hang']],
                    'nhom_hang_cap_1': [product['nhom_hang_cap_1']],
                    'nhom_hang_cap_2': [product['nhom_hang_cap_2']],
                    'daily_quantity': [0],
                    'daily_revenue': [0]
                })
                
                # L·∫•y l·ªãch s·ª≠ g·∫ßn nh·∫•t cho lag features
                hist_query = f"""
                SELECT * FROM retail_dw.fact_transactions
                WHERE chi_nhanh = '{product['chi_nhanh']}'
                  AND ma_hang = '{product['ma_hang']}'
                  AND ngay >= today() - 35
                ORDER BY ngay DESC
                LIMIT 35
                """
                history = self.ch.query(hist_query)
                
                if len(history) > 0:
                    features = pd.concat([history, features], ignore_index=True)
                    features['ngay'] = pd.to_datetime(features['ngay'])
                    
                    features = self.create_features(features)
                    pred_features = features[features['ngay'] == date]
                    
                    if len(pred_features) > 0 and 'product_quantity' in self.models:
                        # L·∫•y features c√≥ s·∫µn, fill missing b·∫±ng 0
                        available_cols = [c for c in self.feature_cols if c in pred_features.columns]
                        X_pred = pred_features[available_cols].fillna(0)
                        
                        # Th√™m missing columns v·ªõi gi√° tr·ªã 0
                        for col in self.feature_cols:
                            if col not in X_pred.columns:
                                X_pred[col] = 0
                        X_pred = X_pred[self.feature_cols]
                        
                        quantity_pred = self.models['product_quantity'].predict(X_pred)[0]
                        revenue_pred = self.models['product_revenue'].predict(X_pred)[0]
                        
                        forecasts.append({
                            'forecast_date': date,
                            'chi_nhanh': product['chi_nhanh'],
                            'ma_hang': product['ma_hang'],
                            'nhom_hang_cap_1': product['nhom_hang_cap_1'],
                            'predicted_quantity': max(0, round(quantity_pred)),
                            'predicted_revenue': max(0, revenue_pred),
                            'confidence_lower': max(0, quantity_pred * 0.8),
                            'confidence_upper': quantity_pred * 1.2,
                            'created_at': datetime.now()
                        })
        
        return pd.DataFrame(forecasts)
    
    def save_forecasts(self, forecasts: pd.DataFrame, send_email: bool = True):
        """L∆∞u d·ª± b√°o v√†o database v√† g·ª≠i email th√¥ng b√°o"""
        # T·∫°o b·∫£ng n·∫øu ch∆∞a c√≥
        create_table_sql = """
        CREATE TABLE IF NOT EXISTS ml_forecasts (
            id SERIAL PRIMARY KEY,
            forecast_date DATE NOT NULL,
            chi_nhanh VARCHAR(100),
            ma_hang VARCHAR(50),
            nhom_hang_cap_1 VARCHAR(200),
            predicted_quantity FLOAT,
            predicted_revenue DECIMAL(15,2),
            confidence_lower FLOAT,
            confidence_upper FLOAT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
        );
        
        CREATE INDEX IF NOT EXISTS idx_forecasts_date ON ml_forecasts(forecast_date);
        CREATE INDEX IF NOT EXISTS idx_forecasts_product ON ml_forecasts(ma_hang);
        """
        
        from sqlalchemy import text
        with self.pg.get_connection() as conn:
            conn.execute(text(create_table_sql))
            
            # Insert forecasts
            forecasts.to_sql('ml_forecasts', conn, if_exists='append', index=False)
        
        logger.info(f"Saved {len(forecasts)} forecasts to database")
        
        # G·ª≠i email th√¥ng b√°o k·∫øt qu·∫£ d·ª± b√°o
        if send_email and self.email_notifier and len(forecasts) > 0:
            try:
                logger.info("üìß ƒêang g·ª≠i email forecast report...")
                
                # L·∫•y m·ªôt s·ªë khuy·∫øn ngh·ªã t·ªìn kho cho top products
                inventory_recs = []
                if 'ma_hang' in forecasts.columns:
                    top_products = forecasts.groupby('ma_hang')['predicted_quantity'].sum().sort_values(ascending=False).head(10)
                    for product_code in top_products.index:
                        try:
                            rec = self.get_inventory_recommendations(product_code)
                            if 'error' not in rec:
                                inventory_recs.append(rec)
                        except:
                            pass
                
                success = self.email_notifier.send_forecast_report(
                    forecasts=forecasts,
                    inventory_recommendations=inventory_recs,
                    model_dir=self.model_dir
                )
                if success:
                    logger.info("‚úÖ ƒê√£ g·ª≠i email forecast report th√†nh c√¥ng")
                else:
                    logger.warning("‚ö†Ô∏è Kh√¥ng th·ªÉ g·ª≠i email forecast report")
            except Exception as e:
                logger.error(f"‚ùå L·ªói khi g·ª≠i email forecast: {e}")
    
    def get_inventory_recommendations(self, product_code: str) -> Dict:
        """ƒê∆∞a ra khuy·∫øn ngh·ªã t·ªìn kho"""
        # L·∫•y d·ª± b√°o cho s·∫£n ph·∫©m
        forecast_query = f"""
        SELECT 
            SUM(predicted_quantity) as total_predicted,
            AVG(predicted_quantity) as avg_daily
        FROM ml_forecasts
        WHERE ma_hang = '{product_code}'
        AND forecast_date >= CURRENT_DATE
        AND forecast_date <= CURRENT_DATE + INTERVAL '7 days'
        """
        
        forecast = self.pg.execute_query(forecast_query)
        
        if forecast.empty:
            return {'error': 'No forecast available'}
        
        total_predicted = forecast['total_predicted'].iloc[0]
        avg_daily = forecast['avg_daily'].iloc[0]
        
        # Ki·ªÉm tra gi√° tr·ªã NULL
        if pd.isna(total_predicted) or pd.isna(avg_daily):
            return {'error': 'Invalid forecast data'}
        
        # Khuy·∫øn ngh·ªã
        return {
            'product_code': product_code,
            'predicted_next_7_days': total_predicted,
            'avg_daily_demand': avg_daily,
            'recommended_safety_stock': round(avg_daily * 7 * 1.5),  # 7 days * safety factor
            'reorder_point': round(avg_daily * 14),  # 2 weeks
            'suggested_order_quantity': round(avg_daily * 30),  # 1 month
            'reorder_urgency': 'High' if total_predicted > avg_daily * 14 else 'Normal'
        }
    
    def send_error_notification(self, error_message: str, context: str = ""):
        """G·ª≠i th√¥ng b√°o l·ªói qua email"""
        if self.email_notifier:
            try:
                self.email_notifier.send_error_alert(error_message, context)
            except Exception as e:
                logger.error(f"Kh√¥ng th·ªÉ g·ª≠i email l·ªói: {e}")


if __name__ == '__main__':
    forecaster = SalesForecaster()
    
    # Train models
    metrics = forecaster.train_all_models()
    print(f"Training metrics: {metrics}")
    
    # Generate forecasts
    forecasts = forecaster.predict_next_week()
    forecaster.save_forecasts(forecasts)
    print(f"Generated {len(forecasts)} forecasts")
