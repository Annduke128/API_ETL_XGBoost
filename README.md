# Retail Data Pipeline - Há»‡ thá»‘ng Data Warehouse cho ngÃ nh bÃ¡n láº»

Há»‡ thá»‘ng data pipeline hoÃ n chá»‰nh vá»›i ETL tá»± Ä‘á»™ng, Data Warehouse, BI Dashboard vÃ  ML Forecasting.

## ğŸ“‹ Má»¥c lá»¥c

1. [Kiáº¿n trÃºc há»‡ thá»‘ng](#kiáº¿n-trÃºc-há»‡-thá»‘ng)
2. [YÃªu cáº§u há»‡ thá»‘ng](#yÃªu-cáº§u-há»‡-thá»‘ng)
3. [CÃ i Ä‘áº·t vÃ  khá»Ÿi Ä‘á»™ng](#cÃ i-Ä‘áº·t-vÃ -khá»Ÿi-Ä‘á»™ng)
4. [Kiá»ƒm tra healthcheck](#kiá»ƒm-tra-healthcheck)
5. [Import dá»¯ liá»‡u CSV](#import-dá»¯-liá»‡u-csv)
6. [Cháº¡y DBT Project](#cháº¡y-dbt-project)
7. [Cháº¡y ML Models](#cháº¡y-ml-models)
8. [TÃ¹y chá»‰nh tham sá»‘](#tÃ¹y-chá»‰nh-tham-sá»‘)
9. [Káº¿t ná»‘i Superset BI](#káº¿t-ná»‘i-superset-bi)
10. [Troubleshooting](#troubleshooting)

---

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  CSV Input  â”‚â”€â”€â”€â”€â–¶â”‚   Cleaner   â”‚â”€â”€â”€â”€â–¶â”‚  PostgreSQL     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚  (OLTP)         â”‚
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
                   â”‚    Redis    â”‚â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                   â”‚   (Buffer)  â”‚              â”‚
                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
                                                â–¼
                                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                        â”‚   ClickHouse    â”‚
                                        â”‚  (Data Warehouse)
                                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                 â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â–²
         â–¼              â–¼                                       â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚   DBT    â”‚   â”‚   ML/XGB â”‚                            â”‚ Superset â”‚
   â”‚Transform â”‚   â”‚ Forecast â”‚                            â”‚   (BI)   â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                                               â–²
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚   Airflow    â”‚
                     â”‚ (Scheduler)  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### CÃ¡c thÃ nh pháº§n chÃ­nh:

| Service | Port | MÃ´ táº£ |
|---------|------|-------|
| **PostgreSQL** | 5432 | OLTP Database - LÆ°u giao dá»‹ch thá»igian thá»±c |
| **ClickHouse** | 8123 (HTTP), 9000 (Native) | Data Warehouse - PhÃ¢n tÃ­ch dá»¯ liá»‡u lá»›n |
| **Redis** | 6379 | Buffer & Cache |
| **Airflow** | 8085 | Workflow Scheduler |
| **Superset** | 8088 | BI Dashboard |

---

## ğŸ’» YÃªu cáº§u há»‡ thá»‘ng

### Tá»‘i thiá»ƒu:
- **RAM**: 8GB
- **Disk**: 20GB free
- **Docker**: 24.0+
- **Docker Compose**: 2.20+

### Khuyáº¿n nghá»‹:
- **RAM**: 16GB+
- **CPU**: 4 cores+
- **Disk**: 50GB+ SSD

---

## ğŸš€ CÃ i Ä‘áº·t vÃ  khá»Ÿi Ä‘á»™ng

### BÆ°á»›c 1: Clone vÃ  vÃ o thÆ° má»¥c

```bash
cd /home/annduke/retail_data_pipeline
```

### BÆ°á»›c 2: Copy file mÃ´i trÆ°á»ng

```bash
cp .env.example .env
```

### BÆ°á»›c 3: Khá»Ÿi Ä‘á»™ng core services

```bash
# Khá»Ÿi Ä‘á»™ng cÃ¡c service chÃ­nh
docker-compose up -d postgres clickhouse redis

# Äá»£i 30-60s cho cÃ¡c service khá»Ÿi Ä‘á»™ng
sleep 30
```

### BÆ°á»›c 4: Khá»Ÿi Ä‘á»™ng Airflow vÃ  Superset

```bash
# Khá»Ÿi Ä‘á»™ng Airflow
docker-compose up -d airflow-init airflow-webserver airflow-scheduler

# Khá»Ÿi Ä‘á»™ng Superset
docker-compose up -d superset-init superset-web

# Äá»£i khá»Ÿi táº¡o xong (khoáº£ng 1-2 phÃºt)
sleep 60
```

### BÆ°á»›c 5: Kiá»ƒm tra táº¥t cáº£ services

```bash
docker-compose ps
```

---

## ğŸ” Kiá»ƒm tra Healthcheck

### 1. Kiá»ƒm tra tá»•ng quan

```bash
# Xem status táº¥t cáº£ containers
docker-compose ps

# Xem logs má»™t service
docker-compose logs -f postgres
docker-compose logs -f clickhouse
docker-compose logs -f superset-web
```

### 2. Kiá»ƒm tra PostgreSQL

```bash
# Test connection
docker-compose exec -T postgres pg_isready -U retail_user -d retail_db

# Xem dá»¯ liá»‡u
docker-compose exec -T postgres psql -U retail_user -d retail_db -c "
  SELECT 
    'branches' as table_name, COUNT(*) as count FROM branches
  UNION ALL
  SELECT 'products', COUNT(*) FROM products
  UNION ALL
  SELECT 'transactions', COUNT(*) FROM transactions;
"
```

### 3. Kiá»ƒm tra ClickHouse

```bash
# Test connection
docker-compose exec -T clickhouse clickhouse-client -q "SELECT 1"

# Xem tables
docker-compose exec -T clickhouse clickhouse-client -q "SHOW TABLES FROM retail_dw"

# Xem dá»¯ liá»‡u
docker-compose exec -T clickhouse clickhouse-client -q "
  SELECT COUNT(*) FROM retail_dw.fact_transactions
"
```

### 4. Kiá»ƒm tra Redis

```bash
# Test ping
docker-compose exec -T redis redis-cli ping

# Xem keys
docker-compose exec -T redis redis-cli KEYS "*"
```

### 5. Kiá»ƒm tra Superset

```bash
# Health check
curl http://localhost:8088/health

# Login page
curl -I http://localhost:8088/login
```

---

## ğŸ“ Import dá»¯ liá»‡u CSV

### Tá»± Ä‘á»™ng (Airflow Schedule)

Há»‡ thá»‘ng tá»± Ä‘á»™ng import CSV **hÃ ng ngÃ y lÃºc 2h sÃ¡ng** thÃ´ng qua Airflow DAG `csv_daily_import`:

```
02:00 AM Daily
    â”œâ”€â”€ QuÃ©t file CSV trong csv_input/
    â”œâ”€â”€ LÃ m sáº¡ch & Import vÃ o PostgreSQL + ClickHouse
    â””â”€â”€ Cháº¡y DBT Transform
```

### Import thá»§ cÃ´ng (Manual)

Náº¿u cáº§n import ngay láº­p tá»©c:

```bash
# Copy file CSV vÃ o thÆ° má»¥c
cp /path/to/your/file.csv csv_input/

# Cháº¡y import (chá»‰ xá»­ lÃ½ 1 láº§n)
make csv-import

# Hoáº·c import + cháº¡y DBT transform
make csv-process-full
```

### Kiá»ƒm tra káº¿t quáº£ import:

```bash
# PostgreSQL
docker-compose exec -T postgres psql -U retail_user -d retail_db -c "
  SELECT 
    'branches' as table_name, COUNT(*) as count FROM branches
  UNION ALL
  SELECT 'products', COUNT(*) FROM products
  UNION ALL
  SELECT 'transactions', COUNT(*) FROM transactions
  UNION ALL
  SELECT 'transaction_details', COUNT(*) FROM transaction_details;
"

# ClickHouse
docker-compose exec -T clickhouse clickhouse-client -q "
  SELECT COUNT(*) FROM retail_dw.fact_transactions
"
```

---

## ğŸ”§ Cháº¡y DBT Project

### Cáº¥u trÃºc DBT:

```
dbt_retail/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/          # LÃ m sáº¡ch dá»¯ liá»‡u gá»‘c
â”‚   â”œâ”€â”€ intermediate/     # Transform trung gian
â”‚   â””â”€â”€ marts/            # Facts & Dimensions
â”‚       â”œâ”€â”€ core/         # dim_date, dim_product, dim_branch
â”‚       â”œâ”€â”€ sales/        # fct_daily_sales, fct_monthly_sales
â”‚       â”œâ”€â”€ inventory/    # fct_inventory_forecast_input
â”‚       â””â”€â”€ customers/    # fct_rfm_analysis
â”œâ”€â”€ seeds/                # Dá»¯ liá»‡u tham chiáº¿u (product.csv, seasonality)
â””â”€â”€ macros/               # HÃ m tiá»‡n Ã­ch
```

### CÃ¡c bÆ°á»›c cháº¡y DBT:

#### 1. CÃ i dependencies

```bash
docker-compose run --rm \
  -e POSTGRES_HOST=postgres \
  -e POSTGRES_PORT=5432 \
  -e POSTGRES_USER=retail_user \
  -e POSTGRES_PASSWORD=retail_password \
  -e POSTGRES_DB=retail_db \
  dbt deps
```

#### 2. Load seeds (dá»¯ liá»‡u tham chiáº¿u)

```bash
docker-compose run --rm \
  -e POSTGRES_HOST=postgres \
  -e POSTGRES_PORT=5432 \
  -e POSTGRES_USER=retail_user \
  -e POSTGRES_PASSWORD=retail_password \
  -e POSTGRES_DB=retail_db \
  dbt seed
```

#### 3. Cháº¡y táº¥t cáº£ models

```bash
docker-compose run --rm \
  -e POSTGRES_HOST=postgres \
  -e POSTGRES_PORT=5432 \
  -e POSTGRES_USER=retail_user \
  -e POSTGRES_PASSWORD=retail_password \
  -e POSTGRES_DB=retail_db \
  dbt run
```

#### 4. Cháº¡y specific models

```bash
# Chá»‰ cháº¡y staging models
docker-compose run --rm \
  -e POSTGRES_HOST=postgres \
  dbt run --select staging

# Cháº¡y má»™t model cá»¥ thá»ƒ
docker-compose run --rm \
  -e POSTGRES_HOST=postgres \
  dbt run --select stg_transactions

# Cháº¡y marts sales
docker-compose run --rm \
  -e POSTGRES_HOST=postgres \
  dbt run --select marts.sales
```

#### 5. Cháº¡y tests

```bash
docker-compose run --rm \
  -e POSTGRES_HOST=postgres \
  dbt test
```

#### 6. Generate vÃ  serve docs

```bash
# Generate docs
docker-compose run --rm \
  -e POSTGRES_HOST=postgres \
  dbt docs generate

# Serve docs (truy cáº­p http://localhost:8080)
docker-compose run --rm \
  -e POSTGRES_HOST=postgres \
  -p 8080:8080 \
  dbt docs serve --host 0.0.0.0 --port 8080
```

---

## ğŸ¤– Cháº¡y ML Models

Há»‡ thá»‘ng sá»­ dá»¥ng **XGBoost** káº¿t há»£p **Optuna** (Bayesian Optimization) Ä‘á»ƒ tá»± Ä‘á»™ng tÃ¬m hyperparameters tá»‘i Æ°u.

### 1. Training vá»›i Optuna Tuning (Recommended)

```bash
# Train vá»›i Optuna (50 trials - default, ~10-15 phÃºt)
make ml-train

# Train tá»‘i Æ°u hÆ¡n (100 trials, ~20-30 phÃºt)
make ml-train-optimal

# Train nhanh (khÃ´ng tuning, ~2-3 phÃºt)
make ml-train-fast

# Train + Generate forecasts
make ml-train-predict
```

### 2. Chi tiáº¿t Hyperparameter Tuning

Há»‡ thá»‘ng tá»± Ä‘á»™ng tÃ¬m kiáº¿m hyperparameters tá»‘i Æ°u:

| Parameter | Range | Ã nghÄ©a |
|-----------|-------|---------|
| `max_depth` | 3-10 | Äá»™ sÃ¢u cÃ¢y |
| `learning_rate` | 0.01-0.3 | Tá»‘c Ä‘á»™ há»c |
| `subsample` | 0.6-1.0 | Sampling ratio |
| `colsample_bytree` | 0.6-1.0 | Feature sampling |
| `reg_alpha/lambda` | 1e-8 - 10 | Regularization |
| `min_child_weight` | 1-10 | Min samples per leaf |

Káº¿t quáº£ tuning Ä‘Æ°á»£c lÆ°u táº¡i:
```
ml_pipeline/models/
â”œâ”€â”€ product_quantity_model.pkl
â”œâ”€â”€ product_revenue_model.pkl
â”œâ”€â”€ category_quantity_model.pkl
â”œâ”€â”€ training_metrics.json          # Metrics & best params
â””â”€â”€ *_optuna_study.pkl            # Optuna studies
```

### 3. Airflow DAG tá»± Ä‘á»™ng

DAG `retail_weekly_ml` sáº½ tá»± Ä‘á»™ng cháº¡y vÃ o 3h sÃ¡ng Chá»§ nháº­t hÃ ng tuáº§n.

Kiá»ƒm tra DAG:
```bash
# VÃ o Airflow UI: http://localhost:8085 (admin/admin)
# Trigger DAG thá»§ cÃ´ng
```

### 3. Xem káº¿t quáº£ dá»± bÃ¡o

```bash
# Trong PostgreSQL
docker-compose exec -T postgres psql -U retail_user -d retail_db -c "
  SELECT * FROM ml_forecasts 
  ORDER BY forecast_date DESC 
  LIMIT 10;
"
```

---

## âš™ï¸ TÃ¹y chá»‰nh tham sá»‘

### 1. Tham sá»‘ trong dbt_project.yml

```yaml
# retail_data_pipeline/dbt_retail/dbt_project.yml

vars:
  # Biáº¿n cho ngÃ nh bÃ¡n láº»
  min_date: '2020-01-01'
  currency: 'VND'
  
  # NgÆ°á»¡ng phÃ¢n loáº¡i giao dá»‹ch
  high_value_threshold: 1000000  # 1 triá»‡u VND
  
  # PhÃ¢n loáº¡i ABC (80/15/5 rule)
  abc_a_threshold: 0.8   # Top 80% doanh thu
  abc_b_threshold: 0.95  # 80-95% doanh thu
  
  # PhÃ¢n loáº¡i RFM
  rfm_recency_high: 7    # NgÃ y
  rfm_recency_medium: 30
  rfm_frequency_high: 10
  rfm_monetary_high: 5000000
```

**Sá»­a xong cháº¡y láº¡i:**
```bash
docker-compose run --rm -e POSTGRES_HOST=postgres dbt run
```

### 2. Tham sá»‘ ML trong xgboost_forecast.py

```python
# retail_data_pipeline/ml_pipeline/xgboost_forecast.py

# CÃ¡ch 1: Sá»­ dá»¥ng Optuna Tuning (Recommended)
# Tá»± Ä‘á»™ng tÃ¬m hyperparameters tá»‘i Æ°u
forecaster.train_all_models(
    use_tuning=True,
    tuning_method='optuna',
    n_trials=50,           # Sá»‘ láº§n thá»­ nghiá»‡m
    days=365               # Sá»‘ ngÃ y lá»‹ch sá»­
)

# CÃ¡ch 2: Manual hyperparameters (nhanh hÆ¡n)
model = xgb.XGBRegressor(
    objective='reg:squarederror',
    n_estimators=500,        # Sá»‘ cÃ¢y
    max_depth=6,             # Äá»™ sÃ¢u tá»‘i Ä‘a
    learning_rate=0.1,       # Tá»‘c Ä‘á»™ há»c
    subsample=0.8,           # Tá»· lá»‡ máº«u
    colsample_bytree=0.8,    # Tá»· lá»‡ cá»™t
    random_state=42
)

# Lead time cho inventory
lead_time_days = 7
```

### 3. Tham sá»‘ CSV Processor

```python
# retail_data_pipeline/data_cleaning/csv_processor.py

# NgÆ°á»¡ng phÃ¢n loáº¡i ABC
abc_a_threshold = 0.8
abc_b_threshold = 0.95

# NgÆ°á»¡ng giÃ¡ trá»‹ giao dá»‹ch
high_value_threshold = 1000000

# Encoding
encoding_priority = ['utf-8', 'utf-8-sig', 'utf-16', 'cp1252']
```

### 4. Tham sá»‘ Docker Compose

```yaml
# retail_data_pipeline/docker-compose.yml

services:
  postgres:
    environment:
      POSTGRES_USER: retail_user      # Thay Ä‘á»•i user
      POSTGRES_PASSWORD: your_pass    # Thay Ä‘á»•i password
      POSTGRES_DB: retail_db          # Thay Ä‘á»•i DB name
    
  clickhouse:
    environment:
      CLICKHOUSE_PASSWORD: your_pass  # Thay Ä‘á»•i password
```

**Apply thay Ä‘á»•i:**
```bash
docker-compose down
docker-compose up -d
```

---

## ğŸ“Š Káº¿t ná»‘i Superset BI

### 1. Truy cáº­p UI

- URL: http://localhost:8088
- Username: `admin`
- Password: `admin`

### 2. ThÃªm Database Connection

**PostgreSQL:**
- Database Name: `Retail PostgreSQL`
- SQLAlchemy URI: `postgresql://retail_user:retail_password@postgres:5432/retail_db`

**ClickHouse:**
- Database Name: `Retail ClickHouse`
- SQLAlchemy URI: `clickhouse+http://default:clickhouse_password@clickhouse:8123/retail_dw`

### 3. Táº¡o Dataset

**Datasets â†’ + Dataset**
- Chá»n Database â†’ Schema â†’ Table
- Tables sáºµn cÃ³ sau khi cháº¡y DBT:
  - `staging.stg_transactions`
  - `marts.fct_daily_sales`
  - `marts.fct_monthly_sales`
  - `marts.fct_rfm_analysis`
  - `marts.dim_product`
  - `marts.dim_branch`

### 4. Táº¡o Chart máº«u

**Chart 1: Doanh thu theo chi nhÃ¡nh**
```
Chart Type: Bar Chart
Dataset: fct_daily_sales
X-axis: chi_nhanh
Metric: SUM(doanh_thu)
```

**Chart 2: Top sáº£n pháº©m bÃ¡n cháº¡y**
```
Chart Type: Table
Dataset: stg_transaction_details
Group by: ten_hang
Metrics: SUM(so_luong), SUM(line_revenue)
Row Limit: 10
Sort: so_luong DESC
```

---

## ğŸ› ï¸ Troubleshooting

### Lá»—i 1: Port Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng

```bash
# Kiá»ƒm tra port
sudo lsof -i :5432
sudo lsof -i :8088

# Thay Ä‘á»•i port trong docker-compose.yml
# VÃ­ dá»¥: thay "8080:8080" thÃ nh "8085:8080"
```

### Lá»—i 2: Container unhealthy

```bash
# Restart container
docker-compose restart clickhouse

# Xem logs chi tiáº¿t
docker-compose logs clickhouse

# Reset hoÃ n toÃ n
docker-compose down -v
docker-compose up -d
```

### Lá»—i 3: DBT connection refused

```bash
# Äáº£m báº£o set Ä‘Ãºng environment variables
docker-compose run --rm \
  -e POSTGRES_HOST=postgres \
  -e POSTGRES_PORT=5432 \
  -e POSTGRES_USER=retail_user \
  -e POSTGRES_PASSWORD=retail_password \
  -e POSTGRES_DB=retail_db \
  dbt run
```

### Lá»—i 4: Redis cache cÅ©

```bash
# XÃ³a cache
docker-compose exec redis redis-cli FLUSHDB

# Hoáº·c restart redis
docker-compose restart redis
```

### Lá»—i 5: CSV Import khÃ´ng cháº¡y

```bash
# Kiá»ƒm tra Airflow DAG cÃ³ Ä‘Æ°á»£c schedule khÃ´ng
curl http://localhost:8085/api/v1/dags/csv_daily_import/dagRuns

# Cháº¡y thá»§ cÃ´ng
make csv-import

# Kiá»ƒm tra file cÃ³ trong thÆ° má»¥c khÃ´ng
ls -la csv_input/

# Kiá»ƒm tra logs
docker-compose logs csv-watcher
```

### Lá»—i 6: Disk full

```bash
# Dá»n dáº¹p Docker
docker system prune -f
docker volume prune -f

# Xem dung lÆ°á»£ng
docker system df
```

---

## ğŸ“ Há»— trá»£

### Commands há»¯u Ã­ch (Makefile)

```bash
make up                 # Khá»Ÿi Ä‘á»™ng táº¥t cáº£ services
make down               # Dá»«ng services
make restart            # Restart
make logs               # Xem logs
make csv-import         # Import CSV thá»§ cÃ´ng
make csv-process-full   # Import CSV + DBT transform
make dbt                # Cháº¡y DBT
make ml                 # Train ML (Optuna 50 trials)
make ml-train-fast      # Train ML nhanh (no tuning)
make ml-predict         # Generate forecasts
make psql               # VÃ o PostgreSQL CLI
make clickhouse         # VÃ o ClickHouse CLI
```

### Kiá»ƒm tra nhanh

```bash
# Script kiá»ƒm tra health
docker-compose ps
docker-compose exec postgres pg_isready -U retail_user
docker-compose exec clickhouse clickhouse-client -q "SELECT 1"
curl http://localhost:8088/health
```

---

## ğŸ“„ License

MIT License

---

**Last Updated:** 2024-02-14
